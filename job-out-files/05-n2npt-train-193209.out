Date:  Wed Jun  1 11:45:11 CDT 2022

Currently Loaded Modules:
  1) intel/18.0.2   4) autotools/1.2   7) TACC
  2) impi/18.0.2    5) cmake/3.16.1    8) python3/3.7.0
  3) git/2.24.1     6) xalt/2.9.6      9) cuda/11.0     (g)

  Where:
   g:  built for GPU

 

Begin batch job... "05-n2npt-train", #193209

Output file: 05-n2npt-train-193209.out 
Partition: gtx 	Nodes: 2 	Ntasks per node: 32
Dataset: HS20MG holes & pillars (60 original imgs), 960/240/7 (train/val/test)

Working directory:  /work/08261/evanat/maverick2/n2n-pytorch

Training parameters: 
  Train dir = hs_20mg_data/train
  Valid dir = hs_20mg_data/valid
  Target dir = hs_20mg_data/targets
  Ckpt save path = ckpts
  Ckpt overwrite = True
  Report interval = 100
  Learning rate = 0.001
  Adam = [0.9, 0.99, 1e-08]
  Batch size = 4
  Nb epochs = 100
  Loss = l2
  Cuda = True
  Plot stats = False
  Noise type = raw
  Noise param = 0.7
  Seed = None
  Crop size = 0
  Clean targets = False
  Paired targets = True

Report interval must be a factor of the total number of batches (nbatches = ntrain / batch_size). 
The report interval has been reset to equal nbatches.

EPOCH 100 / 100
                                                                                Train time: 0:00:06 | Valid time: 0:00:01 | Valid loss: 0.01132 | Avg PSNR: 22.67 dB
Saving checkpoint to: ckpts/05-n2npt-train-raw/n2n-raw.pt

Training done! Total elapsed time: 0:13:46

Batch job runtime was 13 minutes and 58 seconds.
