Date:  Wed Jun 15 13:31:27 CDT 2022Currently Loaded Modules:  1) intel/18.0.2   4) autotools/1.2   7) TACC  2) impi/18.0.2    5) cmake/3.16.1    8) python3/3.7.0  3) git/2.24.1     6) xalt/2.9.6      9) cuda/11.0     (g)  Where:   g:  built for GPU Begin batch job... "08-n2npt-train", #194290Output file: 08-n2npt-train-194290.out Partition: gtx 	Nodes: 2 	Ntasks per node: 32Dataset: B&W, HS20MG holes & pillars (XYZ Files) (60 original imgs), 960/240/7 (train/val/test)Working directory:  /work/08261/evanat/maverick2/n2n-pytorchTraining parameters:   Train dir = hs20mg_xyz_data/train  Valid dir = hs20mg_xyz_data/valid  Target dir = hs20mg_xyz_data/targets  Ckpt save path = ckpts  Ckpt overwrite = True  Report interval = 100  Learning rate = 0.001  Adam = [0.9, 0.99, 1e-08]  Channels = 1  Batch size = 4  Nb epochs = 100  Loss = l2  Cuda = True  Plot stats = False  Noise type = raw  Noise param = 0.7  Seed = None  Crop size = 0  Clean targets = False  Paired targets = TrueReport interval must be a factor of the total number of batches (nbatches = ntrain / batch_size). The report interval has been reset to equal nbatches.                                                                                Train time: 0:01:15 | Valid time: 0:00:10 | Valid loss: 0.00289 | Avg PSNR: 26.48 dBSaving checkpoint to: ckpts/08-n2npt-train-raw/n2n-raw.ptTraining done! Total elapsed time: 2:24:54Batch job runtime was 2h:25m:12s. 