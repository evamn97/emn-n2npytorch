Date:  Mon Jun  6 12:45:50 CDT 2022

Currently Loaded Modules:
  1) intel/18.0.2   4) autotools/1.2   7) TACC
  2) impi/18.0.2    5) cmake/3.16.1    8) python3/3.7.0
  3) git/2.24.1     6) xalt/2.9.6      9) cuda/11.0     (g)

  Where:
   g:  built for GPU

 

Begin batch job... "07-n2npt-train", #193614

Output file: 07-n2npt-train-193614.out 
Partition: gtx 	Nodes: 2 	Ntasks per node: 32
Dataset: B&W, Processed HS20MG holes & pillars (60 original imgs), 960/240/7 (train/val/test) + 4 fast-scan test images

Working directory:  /work/08261/evanat/maverick2/n2n-pytorch

Training parameters: 
  Train dir = processed_hs20mg_data/train
  Valid dir = processed_hs20mg_data/valid
  Target dir = ../data/targets
  Ckpt save path = ckpts
  Ckpt overwrite = True
  Report interval = 100
  Learning rate = 0.001
  Adam = [0.9, 0.99, 1e-08]
  Batch size = 4
  Nb epochs = 100
  Loss = l2
  Cuda = True
  Plot stats = False
  Noise type = lower
  Noise param = 0.25
  Seed = None
  Crop size = 0
  Clean targets = True
  Paired targets = False

Report interval must be a factor of the total number of batches (nbatches = ntrain / batch_size). 
The report interval has been reset to equal nbatches.

Epoch 00054: reducing learning rate of group 0 to 5.0000e-04.
Epoch 00080: reducing learning rate of group 0 to 2.5000e-04.
EPOCH 100 / 100
                                                                                Train time: 0:00:05 | Valid time: 0:00:00 | Valid loss: 0.00047 | Avg PSNR: 35.82 dB
Saving checkpoint to: ckpts/07-n2npt-train-lower/n2n-lower.pt

Training done! Total elapsed time: 0:10:32

Batch job runtime was 10 minutes and 45 seconds.
