{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential decay sinusoidal\n",
    "scratch for an exponentially decaying cosine learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def decay_lr(minlr, maxlr, alpha, beta, E, t):\n",
    "    A = maxlr - minlr\n",
    "    sine_term = 0.5 * A * (np.cos(beta * t * 2 * np.pi) + 1)\n",
    "    exp_term = np.exp(-alpha * t / E)\n",
    "    return minlr + sine_term * exp_term\n",
    "\n",
    "fig1, ax1 = plt.subplots(figsize=(12, 5))\n",
    "epochs = 100\n",
    "min_lr = 0.001\n",
    "max_lr = 0.001\n",
    "alpha = .75\n",
    "beta = .2\n",
    "t = np.arange(epochs, step=0.001)\n",
    "exp_term = decay_lr(min_lr, max_lr, alpha, 0, epochs, t)\n",
    "sine_term = decay_lr(min_lr, max_lr, 0, beta, epochs, t)\n",
    "final = decay_lr(min_lr, max_lr, alpha, beta, epochs, t)\n",
    "ax1.plot(t, exp_term, label='Exponential')\n",
    "ax1.plot(t, sine_term, label='Sinusoid')\n",
    "ax1.plot(t, final, label='Decaying cosine')\n",
    "# ax1.hlines(min(final), 0, epochs, colors='k', label='minimum')\n",
    "ax1.set(ylim=(0, 1.075*max_lr))\n",
    "ax1.legend(loc='upper right')\n",
    "fig1.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Annealing with Warm Restarts\n",
    "\n",
    "<!-- ![](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_5.46.29_PM.png) -->\n",
    "<img align=\"left\" src=https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-30_at_5.46.29_PM.png style=\"width:400px;\"/>\n",
    "\n",
    "#### From *Papers with Code*:\n",
    "Cosine Annealing is a type of learning rate schedule that has the effect of starting with a large learning rate that is relatively rapidly decreased to a minimum value before being increased rapidly again. The resetting of the learning rate acts like a simulated restart of the learning process and the re-use of good weights as the starting point of the restart is referred to as a \"warm restart\" in contrast to a \"cold restart\" where a new set of small random numbers may be used as a starting point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def warm_restarts(eta_min, eta_max, T_0, T_mult, epoch):\n",
    "    if epoch >= T_0:\n",
    "        if T_mult == 1:\n",
    "            T_cur = epoch % T_0\n",
    "        else:\n",
    "            n = int(math.log((epoch / T_0 * (T_mult - 1) + 1), T_mult))\n",
    "            T_cur = epoch - T_0 * (T_mult ** n - 1) / (T_mult - 1)\n",
    "            T_i = T_0 * T_mult ** (n)\n",
    "    else:\n",
    "        T_i = T_0\n",
    "        T_cur = epoch\n",
    "    lr = eta_min + (eta_max - eta_min) * (1 + np.cos(np.pi * T_cur / T_i)) / 2\n",
    "    return lr\n",
    "\n",
    "T_0 = 10\n",
    "T_mult = 2\n",
    "eta_min = 0.0\n",
    "base_lr = 0.001  # eta_max\n",
    "\n",
    "# carst = eta_min + (base_lr - eta_min) * (1 + np.cos(np.pi * T_cur / T_i)) / 2\n",
    "epochs = range(100)\n",
    "carst = [warm_restarts(eta_min, base_lr, T_0, T_mult, epoch=e) for e in epochs]\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(epochs, carst)\n",
    "ax.set(xlabel='T_cur', ylabel='Learning Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Annealing (no warm restarts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def cosine_annealing(T_max, eta_min, eta_max, epoch, group_lr=None):\n",
    "    if group_lr is None:\n",
    "        group_lr = eta_max\n",
    "    if epoch == 0:\n",
    "        return group_lr\n",
    "    elif (epoch - 1 - T_max) % (2 * T_max) == 0:\n",
    "        new_lr = group_lr + 0.5 * (eta_max - eta_min) * (1 - np.cos(np.pi / T_max))\n",
    "    else:\n",
    "        new_lr = eta_min + 0.5 * (eta_max - eta_min) * (1 + np.cos(epoch * np.pi / T_max))\n",
    "    return new_lr\n",
    "\n",
    "T_max = 100\n",
    "epochs = range(1000)\n",
    "eta_min = 0.0\n",
    "eta_max = 0.01\n",
    "\n",
    "cosann = [cosine_annealing(T_max, eta_min, eta_max, 0)]\n",
    "\n",
    "for e in epochs[1:]:\n",
    "    # print(f'cosann[-1] = {cosann[-1]}')\n",
    "    cosann.append(cosine_annealing(T_max, eta_min, eta_max, e, cosann[-1]))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(epochs, cosann)\n",
    "ax.set(xlabel='T_cur', ylabel='Learning Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_max = 10\n",
    "for epoch in range(100):\n",
    "    if (epoch - 1 - T_max) % (2 * T_max) == 0:\n",
    "        print(f'True for {epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.metrics import *\n",
    "import os\n",
    "from src.utils import import_spm, plot_tensors\n",
    "\n",
    "metric = SSIM(data_range=1.0)\n",
    "folder = \"results/denoisedredux0.9-raw-02061403/\"\n",
    "all_results = [f for f in os.listdir(folder) if \"montage\" not in f]\n",
    "ins = [import_spm(os.path.join(folder, i))[1] for i in all_results if \"noisy\" in i]\n",
    "preds = [import_spm(os.path.join(folder, p))[1] for p in all_results if \"denoised\" in p]\n",
    "targs = [import_spm(os.path.join(folder, t))[1] for t in all_results if \"target\" in t]\n",
    "for p, t in zip(preds, targs):\n",
    "    metric.update((p.unsqueeze(dim=0), t.unsqueeze(dim=0)))\n",
    "    val = metric.compute()\n",
    "    print(f'updated result = {val}')\n",
    "    # plot_tensors(p, t, titles=['Pred', 'Targ'], small_fig=True)\n",
    "\n",
    "# metric.reset()\n",
    "print(f'final result = {metric.compute()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import random\n",
    "# import torchvision.transforms.functional as F\n",
    "# from torch import Tensor\n",
    "\n",
    "# from typing import Union\n",
    "\n",
    "# class FunctionalRandomResizedCrop(torch.nn.Module):\n",
    "#     \"\"\" Random square cropping class that sets a fixed crop location for cropping pairs of images.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, size: int, top: int, left: int):\n",
    "#         super().__init__()\n",
    "#         # if type(size) is tuple and len(size) > 2:\n",
    "#         #     raise ValueError('Please only provide two dimensions (h, w) for size. ')\n",
    "#         # elif type(size) is int:\n",
    "#         #     self.size = (size, size)\n",
    "#         # else:\n",
    "#         self.size = size\n",
    "#         self.top = top\n",
    "#         self.left = left\n",
    "\n",
    "#     def forward(self, img):\n",
    "#         _, h, w = F.get_dimensions(img)\n",
    "#         if self.size > h or self.size > w:\n",
    "\n",
    "# class CosineExpDecayLR(LRScheduler):\n",
    "#     def adjust_lr(optimizer, epoch, nb_epochs, learning_params=(0.0, 0.001, 6.5, 10.0), decay=True):\n",
    "#     \"\"\" Adjusts optimizer learning rate to a sine wave.\n",
    "#     :param optimizer: optimizer object\n",
    "#     :param epoch: current epoch\n",
    "#     :param learning_params: [min, max, alpha, beta] learning rate parameters\n",
    "#     \"\"\"\n",
    "#     lr_min = learning_params[0]\n",
    "#     A = learning_params[1] - learning_params[0]\n",
    "#     alpha = learning_params[2]\n",
    "#     beta = learning_params[3]\n",
    "#     sine_term = 0.5 * A * (np.cos(beta * epoch * 2 * np.pi) + 1)\n",
    "#     exp_term = np.exp(-alpha * epoch / nb_epochs)\n",
    "\n",
    "#     if decay:\n",
    "#         lr_new = lr_min + sine_term * exp_term\n",
    "#     else:\n",
    "#         lr_new = lr_min + sine_term\n",
    "    \n",
    "#     for group in optimizer.param_groups:\n",
    "#         group['lr'] = lr_new\n",
    "#     return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# file = \"ckpts/tinyimagenet-sinelr-ssim-raw/tinyimagenet-sinelr-ssim-rawl2/n2n-stats.json\"\n",
    "file = \"ckpts/scheduler-tests/tinyimagenet-cawrlr-ssimredux0.99-rawl2/n2n-stats.json\"\n",
    "train_stats = pd.read_json(file)\n",
    "\n",
    "def trainvalid_metric_plots(train_metric, valid_metric, metric_name):\n",
    "    fig, ax = plt.subplots(dpi=200)\n",
    "    if 'loss' in metric_name:\n",
    "        if train_metric is not None and abs(train_metric[0]) > 100 * abs(train_metric[1]):\n",
    "            temp_t = train_metric[1:]\n",
    "            temp_v = valid_metric[1:]\n",
    "            ylim=(0.98*min(train_metric.min(), valid_metric.min()), 1.02*max(temp_t.max(), temp_v.max()))\n",
    "            ax.set_ylim(ylim[0], ylim[1])\n",
    "            ax.text(0, 0.985*ylim[1], \n",
    "                    '*Note: epoch 1 value(s) out of bounds', \n",
    "                    fontsize='xx-small')\n",
    "        elif abs(valid_metric[0]) > 100 * abs(valid_metric[1]):\n",
    "            temp_v = valid_metric[1:]\n",
    "            ylim=(0.98*valid_metric.min(), 1.02*temp_v.max())\n",
    "            ax.set_ylim(ylim[0], ylim[1])\n",
    "            ax.text(0, 0.985*ylim[1], \n",
    "                    '*Note: epoch 1 value(s) out of bounds', \n",
    "                    fontsize='xx-small')\n",
    "    if train_metric is not None:\n",
    "        ax.plot(range(1, len(train_metric) + 1), train_metric, label=f'Train {metric_name}')\n",
    "    ax.plot(range(1, len(valid_metric) + 1), valid_metric, label=f'Valid {metric_name}')\n",
    "    ax.set(xlabel='Epoch',\n",
    "           ylabel=f'{metric_name}',\n",
    "           title=f\"{'Train and Valid' if train_metric is not None else 'Valid'} {metric_name}\")\n",
    "    ax.legend(loc='upper right')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "trainvalid_metric_plots(train_stats['train_loss'], train_stats['valid_loss'], 'L2 - SSIMs + 1 loss')\n",
    "# trainvalid_metric_plots('.', None, train_stats['train_loss'], 'L2 - SSIMs + 1 loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as trf\n",
    "from torchvision.transforms import functional as tvF\n",
    "from time import sleep\n",
    "from PIL import Image\n",
    "from pathos.helpers import cpu_count\n",
    "from pathos.pools import ProcessPool as Pool\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_image_import(filepath):\n",
    "    with Image.open(filepath) as im_pil:\n",
    "            im_pil.load()\n",
    "    im_tensor = tvF.pil_to_tensor(im_pil)\n",
    "    return os.path.basename(filepath), im_tensor    #(im_tensor, im_pil)\n",
    "\n",
    "\n",
    "def crop_and_remove_padded(in_dict_item, out_dict, crop_size, pad_thresh=0.05):\n",
    "    fname, im_tensor = in_dict_item\n",
    "    c, h, w = im_tensor.shape\n",
    "    white = (1 - pad_thresh) * im_tensor.max()   # get white padding threshold\n",
    "    black = im_tensor.min() * (1 + pad_thresh)   # get black padding threshold\n",
    "\n",
    "    # check for padding using top left and bottom right corners\n",
    "    if not any([torch.all(im_tensor[:, :3, :3] < black), \n",
    "           torch.all(im_tensor[:, :3, :3] > white), \n",
    "           torch.all(im_tensor[:, -3:, -3:] < black), \n",
    "           torch.all(im_tensor[:, -3:, -3:] > white)]):\n",
    "        # if it doesn't have padding, crop to output size with random resized crop (in case crop size is bigger)\n",
    "        cropper = trf.RandomResizedCrop(crop_size)\n",
    "        if any([h != crop_size, w != crop_size]):\n",
    "            out_dict[fname] = cropper(im_tensor)\n",
    "        else:\n",
    "            out_dict[fname] = im_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading  images: 100%|██████████| 28/28 [00:01<00:00, 26.74img/s]\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"results/denoisedredux0.9-raw-02061403/\"\n",
    "save = \".\"\n",
    "\n",
    "supported = ['.png', '.jpg', '.jpeg']\n",
    "source_iter = tqdm([f for f in os.listdir(root_dir) if os.path.splitext(f)[1].lower() in supported], \n",
    "                    desc=f'Loading {os.path.basename(root_dir)} images', unit='img')\n",
    "images_in = {name: obj for (name, obj) in [simple_image_import(os.path.join(root_dir, s)) for s in source_iter]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [min(t.shape[1:]) for _, t in images_in.items()]\n",
    "images_out = {}\n",
    "\n",
    "# using looping\n",
    "min(sizes)\n",
    "images_out = {}\n",
    "for fname, im_tensor in images_in.items():\n",
    "    crop_and_remove_padded((fname, im_tensor), images_out, min(sizes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = images_in['HS-20MG-raw-montage.png']\n",
    "test_topleft = test[:, :3, :3]\n",
    "boolean_test = torch.where(test_topleft > 250)\n",
    "booltest2 = torch.all(test_topleft > 250).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{<TrainerFn.FITTING: 'fit'>: datetime.datetime(2024, 2, 22, 15, 8, 28, 506284), <TrainerFn.VALIDATING: 'validate'>: datetime.datetime(2024, 2, 22, 15, 8, 28, 506293), <TrainerFn.TESTING: 'test'>: datetime.datetime(2024, 2, 22, 15, 8, 28, 506295), <TrainerFn.PREDICTING: 'predict'>: datetime.datetime(2024, 2, 22, 15, 8, 28, 506297)}]\n",
      "0:00:00\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.trainer.states import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "test = {state: datetime.now() for state in TrainerFn}\n",
    "print([test])\n",
    "test['fit']\n",
    "\n",
    "test2 = timedelta(0)\n",
    "print(test2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "n2n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
